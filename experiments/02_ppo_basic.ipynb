{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 02. PPO基礎 - Proximal Policy Optimizationによる強化学習\n",
    "\n",
    "このノートブックでは、PPO（Proximal Policy Optimization）アルゴリズムを使ってCartPole環境でエージェントを訓練します。\n",
    "\n",
    "## PPOとは\n",
    "\n",
    "PPO（Proximal Policy Optimization）は、方策勾配法に基づく強化学習アルゴリズムです：\n",
    "\n",
    "- **方策勾配法**: 方策（行動選択の確率分布）を直接最適化\n",
    "- **Clipped Surrogate Objective**: 方策の更新幅を制限し、安定した学習を実現\n",
    "- **Actor-Critic**: 方策（Actor）と価値関数（Critic）を同時に学習\n",
    "- **オンポリシー**: 現在の方策で収集したデータを使って学習\n",
    "\n",
    "## DQNとの違い\n",
    "\n",
    "| 特徴 | DQN | PPO |\n",
    "|------|-----|-----|\n",
    "| アプローチ | 価値ベース（Q値を学習） | 方策ベース（方策を直接学習） |\n",
    "| データ効率 | オフポリシー（過去のデータを再利用） | オンポリシー（現在の方策のデータのみ） |\n",
    "| 行動空間 | 離散行動のみ | 離散・連続両方に対応 |\n",
    "| 学習の安定性 | Experience Replay + Target Network | Clipped Objective |\n",
    "| サンプル効率 | 高い（データ再利用） | 低い（データ使い捨て） |\n",
    "\n",
    "## 要件\n",
    "- 要件 1.3: CartPoleでPPOエージェントを訓練し、エピソード報酬の学習曲線を記録、DQNとの挙動・学習速度を比較"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 必要なライブラリのインポート\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import animation\n",
    "from IPython.display import HTML\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from stable_baselines3.common.callbacks import BaseCallback\n",
    "import warnings\n",
    "import japanize_matplotlib\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"✅ ライブラリのインポート完了\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. CartPole環境の確認\n",
    "\n",
    "PPOで訓練する前に、環境の詳細を確認します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CartPole環境の作成\n",
    "env = gym.make('CartPole-v1', render_mode='rgb_array')\n",
    "\n",
    "print(f\"観測空間: {env.observation_space}\")\n",
    "print(f\"行動空間: {env.action_space}\")\n",
    "print(f\"最大エピソード長: 500ステップ\")\n",
    "print(\"\\n観測の内容:\")\n",
    "print(\"  [0] カート位置 (-4.8 ~ 4.8)\")\n",
    "print(\"  [1] カート速度\")\n",
    "print(\"  [2] ポール角度 (-0.418 ~ 0.418 rad ≈ -24° ~ 24°)\")\n",
    "print(\"  [3] ポール角速度\")\n",
    "print(\"\\n行動:\")\n",
    "print(\"  0: 左に押す\")\n",
    "print(\"  1: 右に押す\")\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 学習曲線記録用のコールバック\n",
    "\n",
    "訓練中のパフォーマンスを記録するためのカスタムコールバックを定義します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LearningCurveCallback(BaseCallback):\n",
    "    \"\"\"\n",
    "    訓練中の学習曲線を記録するコールバック\n",
    "    \n",
    "    定期的にモデルを評価し、平均報酬を記録します。\n",
    "    \"\"\"\n",
    "    def __init__(self, eval_env, eval_freq=2000, n_eval_episodes=5, verbose=1):\n",
    "        super().__init__(verbose)\n",
    "        self.eval_env = eval_env\n",
    "        self.eval_freq = eval_freq\n",
    "        self.n_eval_episodes = n_eval_episodes\n",
    "        self.timesteps = []\n",
    "        self.mean_rewards = []\n",
    "        self.std_rewards = []\n",
    "    \n",
    "    def _on_step(self) -> bool:\n",
    "        # eval_freq回ごとに評価\n",
    "        if self.n_calls % self.eval_freq == 0:\n",
    "            # 現在のモデルを評価\n",
    "            mean_reward, std_reward = evaluate_policy(\n",
    "                self.model,\n",
    "                self.eval_env,\n",
    "                n_eval_episodes=self.n_eval_episodes,\n",
    "                deterministic=True\n",
    "            )\n",
    "            \n",
    "            self.timesteps.append(self.n_calls)\n",
    "            self.mean_rewards.append(mean_reward)\n",
    "            self.std_rewards.append(std_reward)\n",
    "            \n",
    "            if self.verbose > 0:\n",
    "                print(f\"  ステップ {self.n_calls}: 平均報酬 = {mean_reward:.2f} ± {std_reward:.2f}\")\n",
    "        \n",
    "        return True\n",
    "\n",
    "print(\"✅ コールバッククラス定義完了\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. PPOエージェントの訓練\n",
    "\n",
    "PPOアルゴリズムを使ってエージェントを訓練します。\n",
    "\n",
    "### PPOのハイパーパラメータ\n",
    "\n",
    "- **learning_rate**: 学習率（0.0003）\n",
    "- **n_steps**: 各環境で収集するステップ数（2048）\n",
    "- **batch_size**: ミニバッチサイズ（64）\n",
    "- **n_epochs**: 各データセットで学習するエポック数（10）\n",
    "- **gamma**: 割引率（0.99）\n",
    "- **gae_lambda**: GAE（Generalized Advantage Estimation）のλ（0.95）\n",
    "- **clip_range**: Clipped Surrogate Objectiveのクリップ範囲（0.2）\n",
    "- **ent_coef**: エントロピー係数（探索を促進）（0.0）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 訓練用環境の作成\n",
    "train_env = gym.make('CartPole-v1')\n",
    "eval_env = gym.make('CartPole-v1')\n",
    "\n",
    "# PPOモデルの作成\n",
    "print(\"PPOエージェントを訓練中...\")\n",
    "print(\"\\nPPOハイパーパラメータ:\")\n",
    "print(\"  学習率: 0.0003\")\n",
    "print(\"  収集ステップ数: 2048\")\n",
    "print(\"  バッチサイズ: 64\")\n",
    "print(\"  エポック数: 10\")\n",
    "print(\"  割引率: 0.99\")\n",
    "print(\"  GAE λ: 0.95\")\n",
    "print(\"  クリップ範囲: 0.2\\n\")\n",
    "\n",
    "model = PPO(\n",
    "    \"MlpPolicy\",\n",
    "    train_env,\n",
    "    verbose=0,\n",
    "    learning_rate=0.0003,\n",
    "    n_steps=2048,\n",
    "    batch_size=64,\n",
    "    n_epochs=10,\n",
    "    gamma=0.99,\n",
    "    gae_lambda=0.95,\n",
    "    clip_range=0.2,\n",
    "    ent_coef=0.0,\n",
    ")\n",
    "\n",
    "# コールバックの作成\n",
    "callback = LearningCurveCallback(eval_env, eval_freq=2000, n_eval_episodes=5)\n",
    "\n",
    "# 訓練実行（50,000ステップ）\n",
    "model.learn(total_timesteps=50000, callback=callback, progress_bar=True)\n",
    "\n",
    "print(\"\\n✅ 訓練完了\")\n",
    "\n",
    "train_env.close()\n",
    "eval_env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 訓練済みPPOエージェントの評価"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 訓練済みエージェントの評価\n",
    "eval_env = gym.make('CartPole-v1')\n",
    "mean_reward, std_reward = evaluate_policy(\n",
    "    model,\n",
    "    eval_env,\n",
    "    n_eval_episodes=10,\n",
    "    deterministic=True\n",
    ")\n",
    "eval_env.close()\n",
    "\n",
    "print(f\"訓練済みPPOエージェント:\")\n",
    "print(f\"  平均報酬: {mean_reward:.2f} ± {std_reward:.2f}\")\n",
    "print(f\"  最大報酬: 500.00 (CartPole-v1の上限)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 学習曲線の可視化\n",
    "\n",
    "訓練中のパフォーマンス向上を可視化します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 学習曲線のプロット\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# 平均報酬と標準偏差\n",
    "timesteps = np.array(callback.timesteps)\n",
    "mean_rewards = np.array(callback.mean_rewards)\n",
    "std_rewards = np.array(callback.std_rewards)\n",
    "\n",
    "plt.plot(timesteps, mean_rewards, 'g-', linewidth=2, label='平均報酬')\n",
    "plt.fill_between(\n",
    "    timesteps,\n",
    "    mean_rewards - std_rewards,\n",
    "    mean_rewards + std_rewards,\n",
    "    alpha=0.3,\n",
    "    color='green',\n",
    "    label='標準偏差'\n",
    ")\n",
    "\n",
    "# 目標ライン（CartPole-v1の最大報酬）\n",
    "plt.axhline(y=500, color='r', linestyle='--', linewidth=1.5, label='最大報酬 (500)')\n",
    "\n",
    "# グラフの装飾\n",
    "plt.xlabel('訓練ステップ数', fontsize=12)\n",
    "plt.ylabel('エピソード報酬', fontsize=12)\n",
    "plt.title('PPO学習曲線 - CartPole-v1', fontsize=14, fontweight='bold')\n",
    "plt.legend(loc='lower right', fontsize=10)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n学習の進捗:\")\n",
    "print(f\"  初期パフォーマンス: {mean_rewards[0]:.2f}\")\n",
    "print(f\"  最終パフォーマンス: {mean_rewards[-1]:.2f}\")\n",
    "print(f\"  改善率: {((mean_rewards[-1] - mean_rewards[0]) / mean_rewards[0] * 100):.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. 訓練済みエージェントの可視化\n",
    "\n",
    "訓練済みPPOエージェントの実際の動作を確認します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_animation(model, env, n_steps=500):\n",
    "    \"\"\"\n",
    "    エージェントの動作をアニメーション化\n",
    "    \n",
    "    Args:\n",
    "        model: 訓練済みモデル\n",
    "        env: Gym環境\n",
    "        n_steps: 最大ステップ数\n",
    "    \n",
    "    Returns:\n",
    "        animation: matplotlibアニメーション\n",
    "    \"\"\"\n",
    "    frames = []\n",
    "    obs, info = env.reset()\n",
    "    done = False\n",
    "    step = 0\n",
    "    total_reward = 0\n",
    "    \n",
    "    while not done and step < n_steps:\n",
    "        # フレームを記録\n",
    "        frame = env.render()\n",
    "        frames.append(frame)\n",
    "        \n",
    "        # 行動を予測\n",
    "        action, _ = model.predict(obs, deterministic=True)\n",
    "        obs, reward, terminated, truncated, info = env.step(action)\n",
    "        done = terminated or truncated\n",
    "        total_reward += reward\n",
    "        step += 1\n",
    "    \n",
    "    print(f\"エピソード完了: {step}ステップ, 累積報酬: {total_reward:.0f}\")\n",
    "    \n",
    "    # アニメーション作成\n",
    "    fig, ax = plt.subplots(figsize=(8, 6))\n",
    "    ax.axis('off')\n",
    "    img = ax.imshow(frames[0])\n",
    "    \n",
    "    def animate(i):\n",
    "        img.set_data(frames[i])\n",
    "        ax.set_title(f'PPOエージェント - ステップ {i+1}/{len(frames)}', fontsize=12)\n",
    "        return [img]\n",
    "    \n",
    "    anim = animation.FuncAnimation(\n",
    "        fig, animate, frames=len(frames), interval=50, blit=True\n",
    "    )\n",
    "    plt.close()\n",
    "    \n",
    "    return anim\n",
    "\n",
    "# アニメーション作成\n",
    "env = gym.make('CartPole-v1', render_mode='rgb_array')\n",
    "anim = create_animation(model, env)\n",
    "env.close()\n",
    "\n",
    "# アニメーション表示\n",
    "HTML(anim.to_jshtml())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. DQNとPPOの比較\n",
    "\n",
    "DQN（01_dqn_basic.ipynb）とPPOの学習特性を比較します。\n",
    "\n",
    "### 学習速度の比較\n",
    "\n",
    "**理論的な違い:**\n",
    "\n",
    "1. **DQN（オフポリシー）**:\n",
    "   - Experience Replayで過去のデータを再利用\n",
    "   - サンプル効率が高い\n",
    "   - 学習が安定するまで時間がかかる場合がある\n",
    "\n",
    "2. **PPO（オンポリシー）**:\n",
    "   - 現在の方策で収集したデータのみを使用\n",
    "   - サンプル効率は低いが、学習が安定\n",
    "   - 方策を直接最適化するため、収束が速い場合がある\n",
    "\n",
    "### 実際の観察\n",
    "\n",
    "CartPole-v1のような単純な環境では：\n",
    "\n",
    "- **DQN**: 初期の探索フェーズが長く、徐々に性能が向上\n",
    "- **PPO**: より早く安定した性能に到達する傾向\n",
    "\n",
    "### 適用場面\n",
    "\n",
    "| アルゴリズム | 適した場面 |\n",
    "|------------|----------|\n",
    "| DQN | 離散行動空間、サンプル効率重視、オフライン学習 |\n",
    "| PPO | 連続・離散両方、学習の安定性重視、ロボティクス |\n",
    "\n",
    "### 学習曲線の特徴\n",
    "\n",
    "- **DQN**: ε-greedyによる探索期間が明確に見える（初期の不安定さ）\n",
    "- **PPO**: より滑らかな学習曲線、早期に高性能に到達"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DQNとPPOの学習曲線を比較するための可視化\n",
    "# 注: このセルを実行するには、01_dqn_basic.ipynbを先に実行して\n",
    "# DQNの学習データを保存しておく必要があります\n",
    "\n",
    "print(\"\\n=== DQN vs PPO 比較 ===\")\n",
    "print(\"\\n学習アプローチ:\")\n",
    "print(\"  DQN: 価値ベース（Q値を学習）\")\n",
    "print(\"  PPO: 方策ベース（方策を直接学習）\")\n",
    "print(\"\\nデータ効率:\")\n",
    "print(\"  DQN: オフポリシー（過去のデータを再利用）\")\n",
    "print(\"  PPO: オンポリシー（現在の方策のデータのみ）\")\n",
    "print(\"\\n学習の安定性:\")\n",
    "print(\"  DQN: Experience Replay + Target Network\")\n",
    "print(\"  PPO: Clipped Objective\")\n",
    "print(\"\\n行動空間:\")\n",
    "print(\"  DQN: 離散行動のみ\")\n",
    "print(\"  PPO: 離散・連続両方に対応\")\n",
    "print(\"\\nCartPole-v1での観察:\")\n",
    "print(\"  DQN: 探索期間が長く、徐々に性能向上\")\n",
    "print(\"  PPO: より早く安定した性能に到達\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. PPOの特徴と学習の観察\n",
    "\n",
    "### PPOの主な特徴\n",
    "\n",
    "1. **Clipped Surrogate Objective**: 方策の更新幅を制限することで、大きな更新による性能劣化を防止\n",
    "\n",
    "2. **Actor-Critic**: 方策（Actor）と価値関数（Critic）を同時に学習し、効率的な学習を実現\n",
    "\n",
    "3. **GAE（Generalized Advantage Estimation）**: アドバンテージ関数の推定にバイアス-バリアンストレードオフを導入\n",
    "\n",
    "4. **オンポリシー学習**: 現在の方策で収集したデータを使用するため、学習が安定\n",
    "\n",
    "### 学習曲線の観察ポイント\n",
    "\n",
    "- **初期段階（0-10,000ステップ）**: 方策の初期化により、比較的早く良い性能を示す\n",
    "- **中期段階（10,000-30,000ステップ）**: 安定した学習により、徐々に最適な方策に収束\n",
    "- **後期段階（30,000-50,000ステップ）**: 最適な方策に到達し、安定した高報酬を維持\n",
    "\n",
    "### DQNとの比較まとめ\n",
    "\n",
    "- **学習速度**: PPOの方が早く高性能に到達する傾向\n",
    "- **安定性**: PPOの方が学習曲線が滑らか\n",
    "- **サンプル効率**: DQNの方がデータを効率的に利用\n",
    "- **適用範囲**: PPOは連続行動空間にも対応可能\n",
    "\n",
    "### 次のステップ\n",
    "\n",
    "次のノートブック（03_maze_env.ipynb）では、カスタム迷路環境を実装し、より複雑な問題でRLエージェントを訓練します。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. モデルの保存（オプション）\n",
    "\n",
    "訓練済みモデルを保存して、後で再利用できるようにします。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# モデルの保存\n",
    "model_path = \"../ml/models/ppo_cartpole.zip\"\n",
    "model.save(model_path)\n",
    "print(f\"✅ モデルを保存しました: {model_path}\")\n",
    "\n",
    "# モデルの読み込み例\n",
    "# loaded_model = PPO.load(model_path)\n",
    "# print(\"✅ モデルを読み込みました\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## まとめ\n",
    "\n",
    "このノートブックでは、PPOアルゴリズムを使ってCartPole環境でエージェントを訓練し、DQNと比較しました：\n",
    "\n",
    "✅ PPOの基本概念（Clipped Objective、Actor-Critic、GAE）を理解\n",
    "\n",
    "✅ Stable-Baselines3を使ってPPOエージェントを訓練\n",
    "\n",
    "✅ 学習曲線を記録し、訓練中のパフォーマンス向上を可視化\n",
    "\n",
    "✅ 訓練済みエージェントの動作を確認\n",
    "\n",
    "✅ DQNとPPOの学習特性・適用場面を比較\n",
    "\n",
    "### 主な発見\n",
    "\n",
    "- PPOはDQNよりも早く安定した性能に到達\n",
    "- PPOの学習曲線はより滑らか\n",
    "- PPOは連続行動空間にも対応可能で、より汎用的\n",
    "- DQNはサンプル効率が高く、オフライン学習に適している\n",
    "\n",
    "次のステップでは、カスタム迷路環境を実装し、より複雑な問題でRLエージェントを訓練します。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
