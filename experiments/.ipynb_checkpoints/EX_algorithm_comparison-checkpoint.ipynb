{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RL Algorithm Comparison: Random Agent vs DQN vs PPO\n",
    "\n",
    "This notebook provides a comprehensive comparison of three reinforcement learning approaches:\n",
    "- **Random Agent**: Baseline agent that selects actions uniformly at random\n",
    "- **DQN (Deep Q-Network)**: Value-based RL algorithm that learns Q-values\n",
    "- **PPO (Proximal Policy Optimization)**: Policy-based RL algorithm using Actor-Critic architecture\n",
    "\n",
    "## Objectives\n",
    "1. Compare performance metrics (success rate, rewards, episode length)\n",
    "2. Visualize neural network architectures\n",
    "3. Benchmark inference performance\n",
    "4. Understand algorithmic differences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "\n",
    "# Add parent directory to path for imports\n",
    "sys.path.append(str(Path.cwd().parent))\n",
    "\n",
    "# RL libraries\n",
    "import gymnasium as gym\n",
    "from stable_baselines3 import DQN, PPO\n",
    "\n",
    "# Visualization\n",
    "import torch\n",
    "from torchviz import make_dot\n",
    "from torchsummary import summary\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "print(\"✓ All imports successful\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model paths\n",
    "MODEL_DIR = Path('../ml/models')\n",
    "DQN_MODEL_PATH = MODEL_DIR / 'dqn_maze.zip'\n",
    "PPO_MODEL_PATH = MODEL_DIR / 'ppo_cartpole.zip'\n",
    "\n",
    "# Evaluation settings\n",
    "N_EVAL_EPISODES = 100\n",
    "N_INFERENCE_CALLS = 1000\n",
    "\n",
    "# Environment\n",
    "ENV_NAME = 'CartPole-v1'\n",
    "\n",
    "print(f\"Configuration loaded:\")\n",
    "print(f\"  Environment: {ENV_NAME}\")\n",
    "print(f\"  Evaluation episodes: {N_EVAL_EPISODES}\")\n",
    "print(f\"  Inference benchmark calls: {N_INFERENCE_CALLS}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algorithm Explanations\n",
    "\n",
    "Before diving into the comparison, let's understand the fundamental differences between the three approaches we'll be evaluating."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Agent (Baseline)\n",
    "\n",
    "The **Random Agent** serves as our baseline for comparison. It doesn't learn anything - it simply selects actions uniformly at random from the available action space.\n",
    "\n",
    "**Key Characteristics:**\n",
    "- **No learning**: Actions are selected randomly without any policy\n",
    "- **No neural network**: No parameters to train or optimize\n",
    "- **Exploration only**: Pure exploration with no exploitation\n",
    "- **Performance**: Typically poor, but provides a lower bound for comparison\n",
    "\n",
    "**Use Cases:**\n",
    "- Baseline for measuring learning progress\n",
    "- Sanity check that trained agents outperform random behavior\n",
    "- Initial exploration in completely unknown environments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DQN (Deep Q-Network) - Value-Based Method\n",
    "\n",
    "**DQN** is a value-based reinforcement learning algorithm that learns to estimate Q-values (action-values) for each state-action pair. It was introduced by DeepMind in 2015 and marked a breakthrough in applying deep learning to RL.\n",
    "\n",
    "**Core Concept:**\n",
    "- Learns a Q-function: Q(s, a) → expected cumulative reward for taking action 'a' in state 's'\n",
    "- Uses a neural network (Q-Network) to approximate Q-values\n",
    "- Selects actions by choosing the action with the highest Q-value (greedy policy)\n",
    "\n",
    "**Key Components:**\n",
    "1. **Q-Network**: Neural network that takes state as input and outputs Q-values for all actions\n",
    "2. **Experience Replay**: Stores past experiences (s, a, r, s') in a buffer and samples randomly for training\n",
    "3. **Target Network**: Separate network with frozen weights used to compute target Q-values, updated periodically\n",
    "4. **Epsilon-Greedy Exploration**: Balances exploration (random actions) and exploitation (greedy actions)\n",
    "\n",
    "**Learning Process:**\n",
    "```\n",
    "1. Observe state s\n",
    "2. Select action a using ε-greedy policy (random with probability ε, else argmax Q(s, a))\n",
    "3. Execute action, observe reward r and next state s'\n",
    "4. Store transition (s, a, r, s') in replay buffer\n",
    "5. Sample random batch from replay buffer\n",
    "6. Compute target: y = r + γ * max_a' Q_target(s', a')\n",
    "7. Update Q-Network to minimize: (Q(s, a) - y)²\n",
    "```\n",
    "\n",
    "**Advantages:**\n",
    "- Sample efficient due to experience replay\n",
    "- Stable learning with target network\n",
    "- Works well for discrete action spaces\n",
    "- Deterministic policy after training (no stochasticity)\n",
    "\n",
    "**Limitations:**\n",
    "- Only works with discrete action spaces\n",
    "- Can overestimate Q-values\n",
    "- Requires careful hyperparameter tuning\n",
    "\n",
    "**Best Use Cases:**\n",
    "- Discrete action spaces (e.g., game controls, navigation)\n",
    "- Environments where deterministic policies are preferred\n",
    "- Problems where sample efficiency is important"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PPO (Proximal Policy Optimization) - Policy-Based Method\n",
    "\n",
    "**PPO** is a policy-based reinforcement learning algorithm that directly learns a policy (mapping from states to actions). It was introduced by OpenAI in 2017 and has become one of the most popular RL algorithms due to its simplicity and effectiveness.\n",
    "\n",
    "**Core Concept:**\n",
    "- Learns a policy π(a|s) that outputs a probability distribution over actions given a state\n",
    "- Uses Actor-Critic architecture with two neural networks\n",
    "- Optimizes policy using policy gradient methods with a clipped objective\n",
    "\n",
    "**Key Components:**\n",
    "1. **Actor Network**: Learns the policy π(a|s) - what action to take\n",
    "2. **Critic Network**: Learns the value function V(s) - how good is this state\n",
    "3. **Clipped Objective**: Prevents too large policy updates, ensuring stable learning\n",
    "4. **Advantage Function**: A(s,a) = Q(s,a) - V(s), measures how much better an action is than average\n",
    "\n",
    "**Learning Process:**\n",
    "```\n",
    "1. Collect trajectories using current policy π_old\n",
    "2. Compute advantages A(s, a) for each state-action pair\n",
    "3. Update policy π_new to maximize clipped objective:\n",
    "   L = min(r(θ) * A, clip(r(θ), 1-ε, 1+ε) * A)\n",
    "   where r(θ) = π_new(a|s) / π_old(a|s)\n",
    "4. Update value function V(s) to minimize prediction error\n",
    "5. Repeat for multiple epochs on the same batch\n",
    "```\n",
    "\n",
    "**Advantages:**\n",
    "- Works with both discrete and continuous action spaces\n",
    "- Stable and reliable learning (clipping prevents destructive updates)\n",
    "- Sample efficient (reuses data for multiple epochs)\n",
    "- Stochastic policy enables natural exploration\n",
    "- Simpler to tune than other policy gradient methods\n",
    "\n",
    "**Limitations:**\n",
    "- Can be slower than value-based methods in some environments\n",
    "- Requires more computation per update (two networks)\n",
    "- May converge to local optima\n",
    "\n",
    "**Best Use Cases:**\n",
    "- Continuous action spaces (e.g., robotics, control)\n",
    "- Environments requiring stochastic policies\n",
    "- Problems where stability is more important than sample efficiency\n",
    "- Multi-agent scenarios"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key Differences Summary\n",
    "\n",
    "| Aspect | Random Agent | DQN | PPO |\n",
    "|--------|-------------|-----|-----|\n",
    "| **Learning Type** | None | Value-based | Policy-based |\n",
    "| **What it Learns** | Nothing | Q-values Q(s,a) | Policy π(a\\|s) + Value V(s) |\n",
    "| **Network(s)** | None | Q-Network | Actor + Critic |\n",
    "| **Action Selection** | Random | Greedy (argmax Q) | Sample from policy distribution |\n",
    "| **Exploration** | Always random | ε-greedy | Stochastic policy |\n",
    "| **Action Space** | Any | Discrete only | Discrete or Continuous |\n",
    "| **Update Method** | N/A | Q-learning (TD) | Policy gradient (clipped) |\n",
    "| **Sample Efficiency** | N/A | High (replay buffer) | Medium (multiple epochs) |\n",
    "| **Stability** | N/A | Moderate | High (clipping) |\n",
    "| **Determinism** | Random | Deterministic | Stochastic |\n",
    "\n",
    "**Exploration Strategies:**\n",
    "- **Random Agent**: Pure exploration, no exploitation\n",
    "- **DQN**: ε-greedy - random action with probability ε, otherwise greedy\n",
    "- **PPO**: Stochastic policy naturally explores by sampling from action distribution\n",
    "\n",
    "**Update Mechanisms:**\n",
    "- **DQN**: Q-learning updates using Bellman equation: Q(s,a) ← r + γ max_a' Q(s',a')\n",
    "- **PPO**: Policy gradient updates with clipped objective to prevent large policy changes\n",
    "\n",
    "**When to Choose Which:**\n",
    "- **DQN**: Discrete actions, need deterministic policy, sample efficiency is critical\n",
    "- **PPO**: Continuous actions, need stochastic policy, stability is critical, general-purpose default choice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Loading Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_trained_model(algorithm: str, env_name: str, env):\n",
    "    \"\"\"\n",
    "    Load a trained RL model\n",
    "    \n",
    "    Args:\n",
    "        algorithm: 'dqn' or 'ppo'\n",
    "        env_name: Environment name (e.g., 'cartpole', 'maze')\n",
    "        env: Gymnasium environment for compatibility validation\n",
    "    \n",
    "    Returns:\n",
    "        Loaded model instance\n",
    "    \n",
    "    Raises:\n",
    "        FileNotFoundError: If model file doesn't exist\n",
    "        ValueError: If algorithm is not supported or model is incompatible\n",
    "    \"\"\"\n",
    "    algorithm = algorithm.lower()\n",
    "    env_name = env_name.lower()\n",
    "    \n",
    "    # Validate algorithm\n",
    "    if algorithm not in ['dqn', 'ppo']:\n",
    "        raise ValueError(f\"Unsupported algorithm: {algorithm}. Must be 'dqn' or 'ppo'.\")\n",
    "    \n",
    "    # Construct model path\n",
    "    model_path = MODEL_DIR / f\"{algorithm}_{env_name}.zip\"\n",
    "    \n",
    "    # Check if model file exists\n",
    "    if not model_path.exists():\n",
    "        raise FileNotFoundError(\n",
    "            f\"Model file not found: {model_path}\\n\"\n",
    "            f\"\\nTo train this model, run the appropriate training notebook:\"\n",
    "            f\"\\n  - For DQN: experiments/01_dqn_basic.ipynb\"\n",
    "            f\"\\n  - For PPO: experiments/02_ppo_basic.ipynb\"\n",
    "            f\"\\n\\nOr train using the maze environment: experiments/03_maze_env.ipynb\"\n",
    "        )\n",
    "    \n",
    "    # Load the model\n",
    "    try:\n",
    "        if algorithm == 'dqn':\n",
    "            model = DQN.load(model_path, env=env)\n",
    "        else:  # ppo\n",
    "            model = PPO.load(model_path, env=env)\n",
    "        \n",
    "        # Validate model compatibility with environment\n",
    "        if not _check_model_compatibility(model, env):\n",
    "            raise ValueError(\n",
    "                f\"Model trained for different environment specifications.\\n\"\n",
    "                f\"Model observation space: {model.observation_space}\\n\"\n",
    "                f\"Target observation space: {env.observation_space}\\n\"\n",
    "                f\"Model action space: {model.action_space}\\n\"\n",
    "                f\"Target action space: {env.action_space}\"\n",
    "            )\n",
    "        \n",
    "        print(f\"✓ Successfully loaded {algorithm.upper()} model from {model_path}\")\n",
    "        return model\n",
    "        \n",
    "    except Exception as e:\n",
    "        if isinstance(e, (FileNotFoundError, ValueError)):\n",
    "            raise\n",
    "        raise RuntimeError(\n",
    "            f\"Failed to load model: {str(e)}\\n\"\n",
    "            f\"This may be due to version incompatibility.\\n\"\n",
    "            f\"Try retraining the model with your current library versions.\"\n",
    "        )\n",
    "\n",
    "\n",
    "def _check_model_compatibility(model, env) -> bool:\n",
    "    \"\"\"\n",
    "    Check if model is compatible with the target environment\n",
    "    \n",
    "    Args:\n",
    "        model: Loaded RL model\n",
    "        env: Target gymnasium environment\n",
    "    \n",
    "    Returns:\n",
    "        True if compatible, False otherwise\n",
    "    \"\"\"\n",
    "    # Check observation space compatibility\n",
    "    if model.observation_space != env.observation_space:\n",
    "        return False\n",
    "    \n",
    "    # Check action space compatibility\n",
    "    if model.action_space != env.action_space:\n",
    "        return False\n",
    "    \n",
    "    return True\n",
    "\n",
    "\n",
    "print(\"✓ Model loading utilities defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Agent Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomAgent:\n",
    "    \"\"\"\n",
    "    Simple random action agent for baseline comparison.\n",
    "    Matches Stable-Baselines3 interface for consistency.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, action_space):\n",
    "        \"\"\"\n",
    "        Initialize random agent\n",
    "        \n",
    "        Args:\n",
    "            action_space: Gymnasium action space\n",
    "        \"\"\"\n",
    "        self.action_space = action_space\n",
    "    \n",
    "    def predict(self, observation, state=None, episode_start=None, deterministic=True):\n",
    "        \"\"\"\n",
    "        Return random action from action space\n",
    "        \n",
    "        Args:\n",
    "            observation: Current observation (unused for random agent)\n",
    "            state: RNN state (unused, for interface compatibility)\n",
    "            episode_start: Episode start flag (unused, for interface compatibility)\n",
    "            deterministic: Whether to use deterministic policy (unused for random)\n",
    "        \n",
    "        Returns:\n",
    "            Tuple of (action, state)\n",
    "        \"\"\"\n",
    "        return self.action_space.sample(), state\n",
    "\n",
    "\n",
    "print(\"✓ RandomAgent class defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from typing import Dict, Any\n",
    "\n",
    "\n",
    "def evaluate_agent(agent, env, n_episodes: int = 100) -> Dict[str, float]:\n",
    "    \"\"\"\n",
    "    Evaluate agent performance over multiple episodes\n",
    "    \n",
    "    Args:\n",
    "        agent: RL agent with predict() method (DQN, PPO, or RandomAgent)\n",
    "        env: Gymnasium environment\n",
    "        n_episodes: Number of episodes to evaluate\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary containing:\n",
    "            - mean_reward: Average total reward per episode\n",
    "            - std_reward: Standard deviation of rewards\n",
    "            - success_rate: Proportion of successful episodes\n",
    "            - mean_episode_length: Average number of steps per episode\n",
    "    \"\"\"\n",
    "    episode_rewards = []\n",
    "    episode_lengths = []\n",
    "    successes = []\n",
    "    \n",
    "    for episode in range(n_episodes):\n",
    "        obs, info = env.reset()\n",
    "        done = False\n",
    "        truncated = False\n",
    "        episode_reward = 0.0\n",
    "        episode_length = 0\n",
    "        \n",
    "        while not (done or truncated):\n",
    "            # Get action from agent\n",
    "            action, _ = agent.predict(obs, deterministic=True)\n",
    "            \n",
    "            # Take step in environment\n",
    "            obs, reward, done, truncated, info = env.step(action)\n",
    "            \n",
    "            episode_reward += reward\n",
    "            episode_length += 1\n",
    "        \n",
    "        # Record episode metrics\n",
    "        episode_rewards.append(episode_reward)\n",
    "        episode_lengths.append(episode_length)\n",
    "        \n",
    "        # Determine success (environment-specific)\n",
    "        # For CartPole: success if episode length >= 195\n",
    "        # For other envs: check if 'is_success' in info or use reward threshold\n",
    "        if 'is_success' in info:\n",
    "            successes.append(info['is_success'])\n",
    "        else:\n",
    "            # Heuristic: consider high reward as success\n",
    "            # For CartPole, 195+ steps is considered solved\n",
    "            successes.append(episode_length >= 195)\n",
    "    \n",
    "    # Calculate statistics\n",
    "    mean_reward = float(np.mean(episode_rewards))\n",
    "    std_reward = float(np.std(episode_rewards))\n",
    "    success_rate = float(np.mean(successes))\n",
    "    mean_episode_length = float(np.mean(episode_lengths))\n",
    "    \n",
    "    return {\n",
    "        'mean_reward': mean_reward,\n",
    "        'std_reward': std_reward,\n",
    "        'success_rate': success_rate,\n",
    "        'mean_episode_length': mean_episode_length\n",
    "    }\n",
    "\n",
    "\n",
    "def measure_performance(agent, env, n_calls: int = 1000) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Measure agent inference performance and resource usage\n",
    "    \n",
    "    Args:\n",
    "        agent: RL agent with predict() method\n",
    "        env: Gymnasium environment\n",
    "        n_calls: Number of inference calls to measure\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary containing:\n",
    "            - inference_time_ms: Average inference time per action in milliseconds\n",
    "            - parameter_count: Total number of model parameters (0 for RandomAgent)\n",
    "            - memory_footprint_mb: Estimated memory footprint in MB\n",
    "    \"\"\"\n",
    "    # Get a sample observation\n",
    "    obs, _ = env.reset()\n",
    "    \n",
    "    # Warm-up: run a few predictions to ensure model is loaded\n",
    "    for _ in range(10):\n",
    "        agent.predict(obs, deterministic=True)\n",
    "    \n",
    "    # Measure inference time\n",
    "    start_time = time.perf_counter()\n",
    "    for _ in range(n_calls):\n",
    "        agent.predict(obs, deterministic=True)\n",
    "    end_time = time.perf_counter()\n",
    "    \n",
    "    # Calculate average inference time in milliseconds\n",
    "    total_time_ms = (end_time - start_time) * 1000\n",
    "    inference_time_ms = total_time_ms / n_calls\n",
    "    \n",
    "    # Count parameters (if model has a policy network)\n",
    "    parameter_count = 0\n",
    "    memory_footprint_mb = 0.0\n",
    "    \n",
    "    if hasattr(agent, 'policy'):\n",
    "        # For Stable-Baselines3 models (DQN, PPO)\n",
    "        for param in agent.policy.parameters():\n",
    "            parameter_count += param.numel()\n",
    "        \n",
    "        # Estimate memory footprint (parameters * 4 bytes for float32)\n",
    "        memory_footprint_mb = (parameter_count * 4) / (1024 * 1024)\n",
    "    \n",
    "    return {\n",
    "        'inference_time_ms': float(inference_time_ms),\n",
    "        'parameter_count': int(parameter_count),\n",
    "        'memory_footprint_mb': float(memory_footprint_mb)\n",
    "    }\n",
    "\n",
    "\n",
    "print(\"✓ Evaluation engine functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Architecture Extraction and Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, List, Any\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "def extract_architecture(model) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Extract architecture information from a trained RL model\n",
    "    \n",
    "    Args:\n",
    "        model: Trained RL model (DQN or PPO from Stable-Baselines3)\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary containing:\n",
    "            - algorithm: Algorithm name ('DQN' or 'PPO')\n",
    "            - networks: Dict of network information (Q-Network for DQN, Actor/Critic for PPO)\n",
    "            - total_parameters: Total number of parameters across all networks\n",
    "            - trainable_parameters: Number of trainable parameters\n",
    "    \"\"\"\n",
    "    if not hasattr(model, 'policy'):\n",
    "        raise ValueError(\"Model must have a 'policy' attribute (Stable-Baselines3 model)\")\n",
    "    \n",
    "    # Determine algorithm type\n",
    "    algorithm = type(model).__name__\n",
    "    \n",
    "    networks = {}\n",
    "    total_params = 0\n",
    "    trainable_params = 0\n",
    "    \n",
    "    if algorithm == 'DQN':\n",
    "        # DQN has a Q-Network\n",
    "        q_net = model.policy.q_net\n",
    "        q_net_info = _extract_network_info(q_net, 'Q-Network')\n",
    "        networks['q_network'] = q_net_info\n",
    "        total_params += q_net_info['parameter_count']\n",
    "        trainable_params += q_net_info['trainable_parameters']\n",
    "        \n",
    "    elif algorithm == 'PPO':\n",
    "        # PPO has Actor (policy) and Critic (value) networks\n",
    "        # Extract actor network\n",
    "        if hasattr(model.policy, 'mlp_extractor'):\n",
    "            # MlpPolicy structure\n",
    "            actor_net = model.policy.mlp_extractor.policy_net\n",
    "            critic_net = model.policy.mlp_extractor.value_net\n",
    "            \n",
    "            actor_info = _extract_network_info(actor_net, 'Actor')\n",
    "            critic_info = _extract_network_info(critic_net, 'Critic')\n",
    "            \n",
    "            networks['actor'] = actor_info\n",
    "            networks['critic'] = critic_info\n",
    "            \n",
    "            total_params += actor_info['parameter_count'] + critic_info['parameter_count']\n",
    "            trainable_params += actor_info['trainable_parameters'] + critic_info['trainable_parameters']\n",
    "        else:\n",
    "            # Fallback: extract from full policy\n",
    "            policy_info = _extract_network_info(model.policy, 'Policy')\n",
    "            networks['policy'] = policy_info\n",
    "            total_params += policy_info['parameter_count']\n",
    "            trainable_params += policy_info['trainable_parameters']\n",
    "    \n",
    "    return {\n",
    "        'algorithm': algorithm,\n",
    "        'networks': networks,\n",
    "        'total_parameters': total_params,\n",
    "        'trainable_parameters': trainable_params\n",
    "    }\n",
    "\n",
    "\n",
    "def _extract_network_info(network: nn.Module, network_type: str) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Extract detailed information from a PyTorch neural network\n",
    "    \n",
    "    Args:\n",
    "        network: PyTorch neural network module\n",
    "        network_type: Type of network (e.g., 'Q-Network', 'Actor', 'Critic')\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary containing:\n",
    "            - network_type: Type of network\n",
    "            - layers: List of layer information\n",
    "            - parameter_count: Total number of parameters\n",
    "            - trainable_parameters: Number of trainable parameters\n",
    "    \"\"\"\n",
    "    layers = []\n",
    "    param_count = 0\n",
    "    trainable_count = 0\n",
    "    \n",
    "    # Iterate through all modules in the network\n",
    "    for name, module in network.named_modules():\n",
    "        # Skip the root module and container modules\n",
    "        if name == '' or isinstance(module, nn.Sequential):\n",
    "            continue\n",
    "        \n",
    "        # Extract layer information\n",
    "        layer_info = {\n",
    "            'name': name if name else type(module).__name__,\n",
    "            'type': type(module).__name__,\n",
    "            'parameters': 0,\n",
    "            'trainable': True\n",
    "        }\n",
    "        \n",
    "        # Count parameters for this layer\n",
    "        layer_params = 0\n",
    "        for param in module.parameters(recurse=False):\n",
    "            layer_params += param.numel()\n",
    "            if param.requires_grad:\n",
    "                trainable_count += param.numel()\n",
    "        \n",
    "        layer_info['parameters'] = layer_params\n",
    "        param_count += layer_params\n",
    "        \n",
    "        # Add activation function info if available\n",
    "        if isinstance(module, (nn.ReLU, nn.Tanh, nn.Sigmoid, nn.LeakyReLU, nn.ELU)):\n",
    "            layer_info['activation'] = type(module).__name__\n",
    "        \n",
    "        # Add shape info for linear and conv layers\n",
    "        if isinstance(module, nn.Linear):\n",
    "            layer_info['input_features'] = module.in_features\n",
    "            layer_info['output_features'] = module.out_features\n",
    "        elif isinstance(module, nn.Conv2d):\n",
    "            layer_info['in_channels'] = module.in_channels\n",
    "            layer_info['out_channels'] = module.out_channels\n",
    "            layer_info['kernel_size'] = module.kernel_size\n",
    "        \n",
    "        layers.append(layer_info)\n",
    "    \n",
    "    return {\n",
    "        'network_type': network_type,\n",
    "        'layers': layers,\n",
    "        'parameter_count': param_count,\n",
    "        'trainable_parameters': trainable_count\n",
    "    }\n",
    "\n",
    "\n",
    "def visualize_architecture(model, input_shape=None):\n",
    "    \"\"\"\n",
    "    Visualize model architecture using torchsummary or torchviz\n",
    "    \n",
    "    Args:\n",
    "        model: Trained RL model (DQN or PPO)\n",
    "        input_shape: Shape of input tensor (optional, for torchsummary)\n",
    "    \"\"\"\n",
    "    import torch\n",
    "    from torchsummary import summary as torch_summary\n",
    "    \n",
    "    algorithm = type(model).__name__\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"{algorithm} Architecture Visualization\")\n",
    "    print(f\"{'='*60}\\n\")\n",
    "    \n",
    "    # Extract and display architecture information\n",
    "    arch_info = extract_architecture(model)\n",
    "    \n",
    "    print(f\"Algorithm: {arch_info['algorithm']}\")\n",
    "    print(f\"Total Parameters: {arch_info['total_parameters']:,}\")\n",
    "    print(f\"Trainable Parameters: {arch_info['trainable_parameters']:,}\\n\")\n",
    "    \n",
    "    # Display network details\n",
    "    for net_name, net_info in arch_info['networks'].items():\n",
    "        print(f\"\\n{net_info['network_type']}:\")\n",
    "        print(f\"  Parameters: {net_info['parameter_count']:,}\")\n",
    "        print(f\"  Layers: {len(net_info['layers'])}\")\n",
    "        print(f\"\\n  Layer Details:\")\n",
    "        \n",
    "        for i, layer in enumerate(net_info['layers'], 1):\n",
    "            layer_desc = f\"    {i}. {layer['type']}\"\n",
    "            \n",
    "            if 'input_features' in layer:\n",
    "                layer_desc += f\" ({layer['input_features']} -> {layer['output_features']})\"\n",
    "            elif 'in_channels' in layer:\n",
    "                layer_desc += f\" ({layer['in_channels']} -> {layer['out_channels']})\"\n",
    "            \n",
    "            if layer['parameters'] > 0:\n",
    "                layer_desc += f\" - {layer['parameters']:,} params\"\n",
    "            \n",
    "            print(layer_desc)\n",
    "    \n",
    "    print(f\"\\n{'='*60}\\n\")\n",
    "\n",
    "\n",
    "def visualize_architecture_graph(model, sample_input=None):\n",
    "    \"\"\"\n",
    "    Generate visual graph of model architecture using torchviz\n",
    "    \n",
    "    Args:\n",
    "        model: Trained RL model (DQN or PPO)\n",
    "        sample_input: Sample input tensor for the model (optional)\n",
    "    \n",
    "    Returns:\n",
    "        Graphviz graph object that can be displayed in Jupyter\n",
    "    \"\"\"\n",
    "    import torch\n",
    "    from torchviz import make_dot\n",
    "    \n",
    "    algorithm = type(model).__name__\n",
    "    \n",
    "    # If no sample input provided, create one based on observation space\n",
    "    if sample_input is None:\n",
    "        obs_shape = model.observation_space.shape\n",
    "        sample_input = torch.randn(1, *obs_shape)\n",
    "    \n",
    "    # Set model to eval mode\n",
    "    model.policy.eval()\n",
    "    \n",
    "    graphs = {}\n",
    "    \n",
    "    if algorithm == 'DQN':\n",
    "        # Visualize Q-Network\n",
    "        with torch.no_grad():\n",
    "            q_values = model.policy.q_net(sample_input)\n",
    "        \n",
    "        # Create graph with gradient tracking\n",
    "        sample_input_grad = sample_input.requires_grad_(True)\n",
    "        q_values_grad = model.policy.q_net(sample_input_grad)\n",
    "        \n",
    "        graph = make_dot(q_values_grad, params=dict(model.policy.q_net.named_parameters()))\n",
    "        graph.attr(label=f'{algorithm} Q-Network Architecture')\n",
    "        graphs['q_network'] = graph\n",
    "        \n",
    "    elif algorithm == 'PPO':\n",
    "        # Visualize Actor and Critic networks\n",
    "        if hasattr(model.policy, 'mlp_extractor'):\n",
    "            # Extract features first\n",
    "            with torch.no_grad():\n",
    "                features = model.policy.extract_features(sample_input)\n",
    "            \n",
    "            # Visualize Actor (policy) network\n",
    "            features_grad = features.requires_grad_(True)\n",
    "            actor_output = model.policy.mlp_extractor.policy_net(features_grad)\n",
    "            \n",
    "            actor_graph = make_dot(actor_output, \n",
    "                                  params=dict(model.policy.mlp_extractor.policy_net.named_parameters()))\n",
    "            actor_graph.attr(label=f'{algorithm} Actor Network Architecture')\n",
    "            graphs['actor'] = actor_graph\n",
    "            \n",
    "            # Visualize Critic (value) network\n",
    "            features_grad2 = features.requires_grad_(True)\n",
    "            critic_output = model.policy.mlp_extractor.value_net(features_grad2)\n",
    "            \n",
    "            critic_graph = make_dot(critic_output,\n",
    "                                   params=dict(model.policy.mlp_extractor.value_net.named_parameters()))\n",
    "            critic_graph.attr(label=f'{algorithm} Critic Network Architecture')\n",
    "            graphs['critic'] = critic_graph\n",
    "    \n",
    "    return graphs\n",
    "\n",
    "\n",
    "print(\"✓ Architecture extraction and visualization functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Agent Loading and Initialization\n",
    "\n",
    "Now let's load or create all three agents for comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create environment\n",
    "env = gym.make(ENV_NAME)\n",
    "\n",
    "print(f\"Environment: {ENV_NAME}\")\n",
    "print(f\"  Observation space: {env.observation_space}\")\n",
    "print(f\"  Action space: {env.action_space}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Random Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Random Agent (no training required)\n",
    "random_agent = RandomAgent(env.action_space)\n",
    "\n",
    "print(\"✓ Random Agent created\")\n",
    "print(\"  Algorithm: Random\")\n",
    "print(\"  Parameters: 0 (no learning)\")\n",
    "print(\"  Policy: Uniform random action selection\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load DQN Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try to load DQN model\n",
    "try:\n",
    "    dqn_agent = load_trained_model('dqn', 'cartpole', env)\n",
    "    \n",
    "    # Display model information\n",
    "    print(\"\\nDQN Model Information:\")\n",
    "    print(f\"  Model path: {DQN_MODEL_PATH}\")\n",
    "    print(f\"  Observation space: {dqn_agent.observation_space}\")\n",
    "    print(f\"  Action space: {dqn_agent.action_space}\")\n",
    "    \n",
    "    # Count parameters\n",
    "    total_params = sum(p.numel() for p in dqn_agent.policy.parameters())\n",
    "    trainable_params = sum(p.numel() for p in dqn_agent.policy.parameters() if p.requires_grad)\n",
    "    print(f\"  Total parameters: {total_params:,}\")\n",
    "    print(f\"  Trainable parameters: {trainable_params:,}\")\n",
    "    print()\n",
    "    \n",
    "except FileNotFoundError as e:\n",
    "    print(f\"⚠ DQN model not found: {e}\")\n",
    "    print(\"\\nTo train a DQN model, run: experiments/01_dqn_basic.ipynb\")\n",
    "    dqn_agent = None\n",
    "except Exception as e:\n",
    "    print(f\"⚠ Error loading DQN model: {e}\")\n",
    "    dqn_agent = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load PPO Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try to load PPO model\n",
    "try:\n",
    "    ppo_agent = load_trained_model('ppo', 'cartpole', env)\n",
    "    \n",
    "    # Display model information\n",
    "    print(\"\\nPPO Model Information:\")\n",
    "    print(f\"  Model path: {PPO_MODEL_PATH}\")\n",
    "    print(f\"  Observation space: {ppo_agent.observation_space}\")\n",
    "    print(f\"  Action space: {ppo_agent.action_space}\")\n",
    "    \n",
    "    # Count parameters\n",
    "    total_params = sum(p.numel() for p in ppo_agent.policy.parameters())\n",
    "    trainable_params = sum(p.numel() for p in ppo_agent.policy.parameters() if p.requires_grad)\n",
    "    print(f\"  Total parameters: {total_params:,}\")\n",
    "    print(f\"  Trainable parameters: {trainable_params:,}\")\n",
    "    print()\n",
    "    \n",
    "except FileNotFoundError as e:\n",
    "    print(f\"⚠ PPO model not found: {e}\")\n",
    "    print(\"\\nTo train a PPO model, run: experiments/02_ppo_basic.ipynb\")\n",
    "    ppo_agent = None\n",
    "except Exception as e:\n",
    "    print(f\"⚠ Error loading PPO model: {e}\")\n",
    "    ppo_agent = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agent Loading Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary of loaded agents\n",
    "agents = {\n",
    "    'Random': random_agent,\n",
    "    'DQN': dqn_agent,\n",
    "    'PPO': ppo_agent\n",
    "}\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"Agent Loading Summary\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for name, agent in agents.items():\n",
    "    status = \"✓ Loaded\" if agent is not None else \"✗ Not available\"\n",
    "    print(f\"{name:15} {status}\")\n",
    "\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Check if we have at least one trained agent\n",
    "if dqn_agent is None and ppo_agent is None:\n",
    "    print(\"\\n⚠ Warning: No trained agents available for comparison.\")\n",
    "    print(\"Please train at least one agent before running the comparison.\")\n",
    "else:\n",
    "    print(f\"\\n✓ Ready to compare {sum(1 for a in agents.values() if a is not None)} agent(s)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Performance Comparison\n",
    "\n",
    "Let's evaluate and compare the performance of all available agents."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate All Agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate each agent\n",
    "evaluation_results = {}\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(f\"Evaluating Agents ({N_EVAL_EPISODES} episodes each)\")\n",
    "print(\"=\"*60)\n",
    "print()\n",
    "\n",
    "for name, agent in agents.items():\n",
    "    if agent is not None:\n",
    "        print(f\"Evaluating {name}...\")\n",
    "        results = evaluate_agent(agent, env, n_episodes=N_EVAL_EPISODES)\n",
    "        evaluation_results[name] = results\n",
    "        \n",
    "        print(f\"  Mean Reward: {results['mean_reward']:.2f} ± {results['std_reward']:.2f}\")\n",
    "        print(f\"  Success Rate: {results['success_rate']*100:.1f}%\")\n",
    "        print(f\"  Mean Episode Length: {results['mean_episode_length']:.1f}\")\n",
    "        print()\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"✓ Evaluation complete\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance Metrics Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Create comparison table\n",
    "if evaluation_results:\n",
    "    df = pd.DataFrame(evaluation_results).T\n",
    "    df.index.name = 'Agent'\n",
    "    \n",
    "    # Format columns\n",
    "    df['mean_reward'] = df['mean_reward'].apply(lambda x: f\"{x:.2f}\")\n",
    "    df['std_reward'] = df['std_reward'].apply(lambda x: f\"{x:.2f}\")\n",
    "    df['success_rate'] = df['success_rate'].apply(lambda x: f\"{x*100:.1f}%\")\n",
    "    df['mean_episode_length'] = df['mean_episode_length'].apply(lambda x: f\"{x:.1f}\")\n",
    "    \n",
    "    # Rename columns for display\n",
    "    df.columns = ['Mean Reward', 'Std Reward', 'Success Rate', 'Mean Episode Length']\n",
    "    \n",
    "    print(\"\\nPerformance Comparison Table:\")\n",
    "    print(df.to_string())\n",
    "else:\n",
    "    print(\"No evaluation results available.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Performance Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if evaluation_results:\n",
    "    # Prepare data for plotting\n",
    "    agent_names = list(evaluation_results.keys())\n",
    "    mean_rewards = [evaluation_results[name]['mean_reward'] for name in agent_names]\n",
    "    std_rewards = [evaluation_results[name]['std_reward'] for name in agent_names]\n",
    "    success_rates = [evaluation_results[name]['success_rate'] * 100 for name in agent_names]\n",
    "    episode_lengths = [evaluation_results[name]['mean_episode_length'] for name in agent_names]\n",
    "    \n",
    "    # Create color scheme\n",
    "    colors = {'Random': '#e74c3c', 'DQN': '#3498db', 'PPO': '#2ecc71'}\n",
    "    bar_colors = [colors.get(name, '#95a5a6') for name in agent_names]\n",
    "    \n",
    "    # Create subplots\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "    fig.suptitle('RL Algorithm Performance Comparison', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # Plot 1: Mean Reward with error bars\n",
    "    ax1 = axes[0, 0]\n",
    "    bars1 = ax1.bar(agent_names, mean_rewards, yerr=std_rewards, \n",
    "                    color=bar_colors, alpha=0.8, capsize=5)\n",
    "    ax1.set_ylabel('Mean Reward', fontsize=12)\n",
    "    ax1.set_title('Average Reward per Episode', fontsize=13, fontweight='bold')\n",
    "    ax1.grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for i, (bar, val, std) in enumerate(zip(bars1, mean_rewards, std_rewards)):\n",
    "        height = bar.get_height()\n",
    "        ax1.text(bar.get_x() + bar.get_width()/2., height + std,\n",
    "                f'{val:.1f}±{std:.1f}',\n",
    "                ha='center', va='bottom', fontsize=10)\n",
    "    \n",
    "    # Plot 2: Success Rate\n",
    "    ax2 = axes[0, 1]\n",
    "    bars2 = ax2.bar(agent_names, success_rates, color=bar_colors, alpha=0.8)\n",
    "    ax2.set_ylabel('Success Rate (%)', fontsize=12)\n",
    "    ax2.set_title('Success Rate', fontsize=13, fontweight='bold')\n",
    "    ax2.set_ylim(0, 105)\n",
    "    ax2.grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    # Add value labels\n",
    "    for bar, val in zip(bars2, success_rates):\n",
    "        height = bar.get_height()\n",
    "        ax2.text(bar.get_x() + bar.get_width()/2., height,\n",
    "                f'{val:.1f}%',\n",
    "                ha='center', va='bottom', fontsize=10)\n",
    "    \n",
    "    # Plot 3: Episode Length\n",
    "    ax3 = axes[1, 0]\n",
    "    bars3 = ax3.bar(agent_names, episode_lengths, color=bar_colors, alpha=0.8)\n",
    "    ax3.set_ylabel('Mean Episode Length', fontsize=12)\n",
    "    ax3.set_title('Average Episode Length (steps)', fontsize=13, fontweight='bold')\n",
    "    ax3.grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    # Add value labels\n",
    "    for bar, val in zip(bars3, episode_lengths):\n",
    "        height = bar.get_height()\n",
    "        ax3.text(bar.get_x() + bar.get_width()/2., height,\n",
    "                f'{val:.1f}',\n",
    "                ha='center', va='bottom', fontsize=10)\n",
    "    \n",
    "    # Plot 4: Comparative Radar Chart (if we have multiple agents)\n",
    "    ax4 = axes[1, 1]\n",
    "    if len(agent_names) > 1:\n",
    "        # Normalize metrics to 0-1 scale for comparison\n",
    "        max_reward = max(mean_rewards) if max(mean_rewards) > 0 else 1\n",
    "        max_length = max(episode_lengths) if max(episode_lengths) > 0 else 1\n",
    "        \n",
    "        normalized_rewards = [r / max_reward for r in mean_rewards]\n",
    "        normalized_success = [s / 100 for s in success_rates]\n",
    "        normalized_lengths = [l / max_length for l in episode_lengths]\n",
    "        \n",
    "        x = np.arange(len(agent_names))\n",
    "        width = 0.25\n",
    "        \n",
    "        ax4.bar(x - width, normalized_rewards, width, label='Reward (norm)', alpha=0.8)\n",
    "        ax4.bar(x, normalized_success, width, label='Success Rate', alpha=0.8)\n",
    "        ax4.bar(x + width, normalized_lengths, width, label='Episode Length (norm)', alpha=0.8)\n",
    "        \n",
    "        ax4.set_ylabel('Normalized Score', fontsize=12)\n",
    "        ax4.set_title('Normalized Metrics Comparison', fontsize=13, fontweight='bold')\n",
    "        ax4.set_xticks(x)\n",
    "        ax4.set_xticklabels(agent_names)\n",
    "        ax4.legend()\n",
    "        ax4.set_ylim(0, 1.1)\n",
    "        ax4.grid(axis='y', alpha=0.3)\n",
    "    else:\n",
    "        ax4.text(0.5, 0.5, 'Multiple agents needed\\nfor comparison',\n",
    "                ha='center', va='center', transform=ax4.transAxes, fontsize=12)\n",
    "        ax4.axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\n✓ Performance visualizations generated\")\n",
    "else:\n",
    "    print(\"No evaluation results to visualize.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance Analysis\n",
    "\n",
    "### Interpretation Guide:\n",
    "\n",
    "**Mean Reward:**\n",
    "- Higher is better\n",
    "- Represents the average cumulative reward per episode\n",
    "- Random agent typically has the lowest reward\n",
    "- Trained agents (DQN/PPO) should significantly outperform random\n",
    "\n",
    "**Success Rate:**\n",
    "- Percentage of episodes that meet the success criteria\n",
    "- For CartPole: episodes lasting 195+ steps are considered successful\n",
    "- Well-trained agents should achieve >90% success rate\n",
    "\n",
    "**Episode Length:**\n",
    "- Average number of steps before episode termination\n",
    "- Longer episodes generally indicate better performance\n",
    "- For CartPole: maximum is 500 steps\n",
    "\n",
    "**Expected Results:**\n",
    "- Random Agent: ~20-25 reward, ~0% success, ~20-25 steps\n",
    "- DQN: ~450-500 reward, ~95-100% success, ~450-500 steps\n",
    "- PPO: ~450-500 reward, ~95-100% success, ~450-500 steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Architecture Comparison\n",
    "\n",
    "Let's examine and compare the neural network architectures of DQN and PPO."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DQN Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if dqn_agent is not None:\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"DQN Architecture Analysis\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Extract architecture information\n",
    "    dqn_arch = extract_architecture(dqn_agent)\n",
    "    \n",
    "    print(f\"\\nAlgorithm: {dqn_arch['algorithm']}\")\n",
    "    print(f\"Total Parameters: {dqn_arch['total_parameters']:,}\")\n",
    "    print(f\"Trainable Parameters: {dqn_arch['trainable_parameters']:,}\")\n",
    "    \n",
    "    # Display Q-Network details\n",
    "    if 'q_network' in dqn_arch['networks']:\n",
    "        q_net = dqn_arch['networks']['q_network']\n",
    "        print(f\"\\n{q_net['network_type']}:\")\n",
    "        print(f\"  Parameters: {q_net['parameter_count']:,}\")\n",
    "        print(f\"  Layers: {len(q_net['layers'])}\")\n",
    "        print(f\"\\n  Layer Details:\")\n",
    "        \n",
    "        for i, layer in enumerate(q_net['layers'], 1):\n",
    "            layer_desc = f\"    {i}. {layer['type']}\"\n",
    "            \n",
    "            if 'input_features' in layer:\n",
    "                layer_desc += f\" ({layer['input_features']} -> {layer['output_features']})\"\n",
    "            elif 'in_channels' in layer:\n",
    "                layer_desc += f\" ({layer['in_channels']} -> {layer['out_channels']})\"\n",
    "            \n",
    "            if layer['parameters'] > 0:\n",
    "                layer_desc += f\" - {layer['parameters']:,} params\"\n",
    "            \n",
    "            print(layer_desc)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "else:\n",
    "    print(\"⚠ DQN agent not available for architecture analysis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PPO Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if ppo_agent is not None:\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"PPO Architecture Analysis\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Extract architecture information\n",
    "    ppo_arch = extract_architecture(ppo_agent)\n",
    "    \n",
    "    print(f\"\\nAlgorithm: {ppo_arch['algorithm']}\")\n",
    "    print(f\"Total Parameters: {ppo_arch['total_parameters']:,}\")\n",
    "    print(f\"Trainable Parameters: {ppo_arch['trainable_parameters']:,}\")\n",
    "    \n",
    "    # Display Actor network details\n",
    "    if 'actor' in ppo_arch['networks']:\n",
    "        actor_net = ppo_arch['networks']['actor']\n",
    "        print(f\"\\n{actor_net['network_type']}:\")\n",
    "        print(f\"  Parameters: {actor_net['parameter_count']:,}\")\n",
    "        print(f\"  Layers: {len(actor_net['layers'])}\")\n",
    "        print(f\"\\n  Layer Details:\")\n",
    "        \n",
    "        for i, layer in enumerate(actor_net['layers'], 1):\n",
    "            layer_desc = f\"    {i}. {layer['type']}\"\n",
    "            \n",
    "            if 'input_features' in layer:\n",
    "                layer_desc += f\" ({layer['input_features']} -> {layer['output_features']})\"\n",
    "            elif 'in_channels' in layer:\n",
    "                layer_desc += f\" ({layer['in_channels']} -> {layer['out_channels']})\"\n",
    "            \n",
    "            if layer['parameters'] > 0:\n",
    "                layer_desc += f\" - {layer['parameters']:,} params\"\n",
    "            \n",
    "            print(layer_desc)\n",
    "    \n",
    "    # Display Critic network details\n",
    "    if 'critic' in ppo_arch['networks']:\n",
    "        critic_net = ppo_arch['networks']['critic']\n",
    "        print(f\"\\n{critic_net['network_type']}:\")\n",
    "        print(f\"  Parameters: {critic_net['parameter_count']:,}\")\n",
    "        print(f\"  Layers: {len(critic_net['layers'])}\")\n",
    "        print(f\"\\n  Layer Details:\")\n",
    "        \n",
    "        for i, layer in enumerate(critic_net['layers'], 1):\n",
    "            layer_desc = f\"    {i}. {layer['type']}\"\n",
    "            \n",
    "            if 'input_features' in layer:\n",
    "                layer_desc += f\" ({layer['input_features']} -> {layer['output_features']})\"\n",
    "            elif 'in_channels' in layer:\n",
    "                layer_desc += f\" ({layer['in_channels']} -> {layer['out_channels']})\"\n",
    "            \n",
    "            if layer['parameters'] > 0:\n",
    "                layer_desc += f\" - {layer['parameters']:,} params\"\n",
    "            \n",
    "            print(layer_desc)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "else:\n",
    "    print(\"⚠ PPO agent not available for architecture analysis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Side-by-Side Architecture Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if dqn_agent is not None and ppo_agent is not None:\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"Architecture Comparison: DQN vs PPO\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Create comparison table\n",
    "    comparison_data = {\n",
    "        'Metric': [\n",
    "            'Algorithm Type',\n",
    "            'Network Structure',\n",
    "            'Total Parameters',\n",
    "            'Trainable Parameters',\n",
    "            'Number of Networks',\n",
    "            'Memory Footprint (MB)'\n",
    "        ],\n",
    "        'DQN': [\n",
    "            'Value-based',\n",
    "            'Q-Network',\n",
    "            f\"{dqn_arch['total_parameters']:,}\",\n",
    "            f\"{dqn_arch['trainable_parameters']:,}\",\n",
    "            '1 (Q-Network)',\n",
    "            f\"{(dqn_arch['total_parameters'] * 4) / (1024 * 1024):.2f}\"\n",
    "        ],\n",
    "        'PPO': [\n",
    "            'Policy-based',\n",
    "            'Actor-Critic',\n",
    "            f\"{ppo_arch['total_parameters']:,}\",\n",
    "            f\"{ppo_arch['trainable_parameters']:,}\",\n",
    "            '2 (Actor + Critic)',\n",
    "            f\"{(ppo_arch['total_parameters'] * 4) / (1024 * 1024):.2f}\"\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    df_comparison = pd.DataFrame(comparison_data)\n",
    "    print(\"\\n\" + df_comparison.to_string(index=False))\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    \n",
    "    # Key differences\n",
    "    print(\"\\nKey Architectural Differences:\")\n",
    "    print(\"\\n1. Network Structure:\")\n",
    "    print(\"   - DQN: Single Q-Network that outputs Q-values for all actions\")\n",
    "    print(\"   - PPO: Separate Actor (policy) and Critic (value) networks\")\n",
    "    \n",
    "    print(\"\\n2. Output:\")\n",
    "    print(\"   - DQN: Q-values for each action (deterministic argmax selection)\")\n",
    "    print(\"   - PPO: Action probabilities (Actor) + State value (Critic)\")\n",
    "    \n",
    "    print(\"\\n3. Complexity:\")\n",
    "    param_ratio = ppo_arch['total_parameters'] / dqn_arch['total_parameters']\n",
    "    print(f\"   - PPO has {param_ratio:.2f}x the parameters of DQN\")\n",
    "    print(f\"   - PPO requires more computation (two forward passes)\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    \n",
    "elif dqn_agent is not None or ppo_agent is not None:\n",
    "    print(\"⚠ Both DQN and PPO agents needed for side-by-side comparison\")\n",
    "else:\n",
    "    print(\"⚠ No trained agents available for architecture comparison\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Architecture Visualization (Optional)\n",
    "\n",
    "Uncomment and run the cells below to generate visual architecture diagrams using torchviz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Visualize DQN architecture\n",
    "# if dqn_agent is not None:\n",
    "#     try:\n",
    "#         dqn_graphs = visualize_architecture_graph(dqn_agent)\n",
    "#         if 'q_network' in dqn_graphs:\n",
    "#             display(dqn_graphs['q_network'])\n",
    "#     except Exception as e:\n",
    "#         print(f\"Could not generate DQN visualization: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Visualize PPO architecture\n",
    "# if ppo_agent is not None:\n",
    "#     try:\n",
    "#         ppo_graphs = visualize_architecture_graph(ppo_agent)\n",
    "#         if 'actor' in ppo_graphs:\n",
    "#             print(\"Actor Network:\")\n",
    "#             display(ppo_graphs['actor'])\n",
    "#         if 'critic' in ppo_graphs:\n",
    "#             print(\"\\nCritic Network:\")\n",
    "#             display(ppo_graphs['critic'])\n",
    "#     except Exception as e:\n",
    "#         print(f\"Could not generate PPO visualization: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Inference Performance Benchmarking\n",
    "\n",
    "Let's measure and compare the inference speed and resource usage of all agents."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Measure Inference Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Measure performance for each agent\n",
    "performance_results = {}\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(f\"Measuring Inference Performance ({N_INFERENCE_CALLS} calls each)\")\n",
    "print(\"=\"*60)\n",
    "print()\n",
    "\n",
    "for name, agent in agents.items():\n",
    "    if agent is not None:\n",
    "        print(f\"Benchmarking {name}...\")\n",
    "        perf = measure_performance(agent, env, n_calls=N_INFERENCE_CALLS)\n",
    "        performance_results[name] = perf\n",
    "        \n",
    "        print(f\"  Inference Time: {perf['inference_time_ms']:.4f} ms/action\")\n",
    "        print(f\"  Parameters: {perf['parameter_count']:,}\")\n",
    "        print(f\"  Memory Footprint: {perf['memory_footprint_mb']:.2f} MB\")\n",
    "        print()\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"✓ Performance benchmarking complete\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference Performance Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if performance_results:\n",
    "    # Create performance comparison table\n",
    "    df_perf = pd.DataFrame(performance_results).T\n",
    "    df_perf.index.name = 'Agent'\n",
    "    \n",
    "    # Format columns\n",
    "    df_perf['inference_time_ms'] = df_perf['inference_time_ms'].apply(lambda x: f\"{x:.4f}\")\n",
    "    df_perf['parameter_count'] = df_perf['parameter_count'].apply(lambda x: f\"{x:,}\")\n",
    "    df_perf['memory_footprint_mb'] = df_perf['memory_footprint_mb'].apply(lambda x: f\"{x:.2f}\")\n",
    "    \n",
    "    # Rename columns for display\n",
    "    df_perf.columns = ['Inference Time (ms)', 'Parameters', 'Memory (MB)']\n",
    "    \n",
    "    print(\"\\nInference Performance Comparison:\")\n",
    "    print(df_perf.to_string())\n",
    "else:\n",
    "    print(\"No performance results available.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Inference Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if performance_results:\n",
    "    # Prepare data for plotting\n",
    "    agent_names = list(performance_results.keys())\n",
    "    inference_times = [performance_results[name]['inference_time_ms'] for name in agent_names]\n",
    "    param_counts = [performance_results[name]['parameter_count'] for name in agent_names]\n",
    "    memory_footprints = [performance_results[name]['memory_footprint_mb'] for name in agent_names]\n",
    "    \n",
    "    # Create color scheme\n",
    "    colors = {'Random': '#e74c3c', 'DQN': '#3498db', 'PPO': '#2ecc71'}\n",
    "    bar_colors = [colors.get(name, '#95a5a6') for name in agent_names]\n",
    "    \n",
    "    # Create subplots\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(16, 5))\n",
    "    fig.suptitle('Inference Performance Comparison', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # Plot 1: Inference Time\n",
    "    ax1 = axes[0]\n",
    "    bars1 = ax1.bar(agent_names, inference_times, color=bar_colors, alpha=0.8)\n",
    "    ax1.set_ylabel('Inference Time (ms)', fontsize=12)\n",
    "    ax1.set_title('Average Inference Time per Action', fontsize=13, fontweight='bold')\n",
    "    ax1.grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    # Add value labels\n",
    "    for bar, val in zip(bars1, inference_times):\n",
    "        height = bar.get_height()\n",
    "        ax1.text(bar.get_x() + bar.get_width()/2., height,\n",
    "                f'{val:.4f}',\n",
    "                ha='center', va='bottom', fontsize=10)\n",
    "    \n",
    "    # Plot 2: Parameter Count\n",
    "    ax2 = axes[1]\n",
    "    bars2 = ax2.bar(agent_names, param_counts, color=bar_colors, alpha=0.8)\n",
    "    ax2.set_ylabel('Number of Parameters', fontsize=12)\n",
    "    ax2.set_title('Model Size (Parameters)', fontsize=13, fontweight='bold')\n",
    "    ax2.grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    # Add value labels\n",
    "    for bar, val in zip(bars2, param_counts):\n",
    "        height = bar.get_height()\n",
    "        ax2.text(bar.get_x() + bar.get_width()/2., height,\n",
    "                f'{val:,}' if val > 0 else '0',\n",
    "                ha='center', va='bottom', fontsize=10, rotation=0 if val < 10000 else 45)\n",
    "    \n",
    "    # Plot 3: Memory Footprint\n",
    "    ax3 = axes[2]\n",
    "    bars3 = ax3.bar(agent_names, memory_footprints, color=bar_colors, alpha=0.8)\n",
    "    ax3.set_ylabel('Memory Footprint (MB)', fontsize=12)\n",
    "    ax3.set_title('Estimated Memory Usage', fontsize=13, fontweight='bold')\n",
    "    ax3.grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    # Add value labels\n",
    "    for bar, val in zip(bars3, memory_footprints):\n",
    "        height = bar.get_height()\n",
    "        ax3.text(bar.get_x() + bar.get_width()/2., height,\n",
    "                f'{val:.2f}' if val > 0 else '0',\n",
    "                ha='center', va='bottom', fontsize=10)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\n✓ Inference performance visualizations generated\")\n",
    "else:\n",
    "    print(\"No performance results to visualize.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance Analysis\n",
    "\n",
    "### Interpretation Guide:\n",
    "\n",
    "**Inference Time:**\n",
    "- Lower is better for real-time applications\n",
    "- Random agent is fastest (no computation, just random sampling)\n",
    "- Neural network agents (DQN/PPO) require forward pass computation\n",
    "- PPO typically slower than DQN (two networks vs one)\n",
    "\n",
    "**Parameter Count:**\n",
    "- Indicates model complexity\n",
    "- More parameters = more capacity but also more computation\n",
    "- Random agent has 0 parameters (no learning)\n",
    "- PPO typically has more parameters than DQN (Actor + Critic)\n",
    "\n",
    "**Memory Footprint:**\n",
    "- Estimated memory required to store model parameters\n",
    "- Important for deployment on resource-constrained devices\n",
    "- Calculated as: parameters × 4 bytes (float32)\n",
    "- Does not include activation memory or batch processing overhead\n",
    "\n",
    "**Typical Results:**\n",
    "- Random Agent: <0.01 ms, 0 params, 0 MB\n",
    "- DQN: 0.1-1 ms, 10k-100k params, 0.04-0.4 MB\n",
    "- PPO: 0.2-2 ms, 20k-200k params, 0.08-0.8 MB\n",
    "\n",
    "**Trade-offs:**\n",
    "- Random Agent: Fastest but worst performance\n",
    "- DQN: Good balance of speed and performance for discrete actions\n",
    "- PPO: Slightly slower but more flexible (works with continuous actions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Learning Curve Comparison (Optional)\n",
    "\n",
    "If training logs are available, we can compare how DQN and PPO learned over time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Training Logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Try to find training logs\n",
    "LOG_DIR = Path('../ml/experiments/logs')\n",
    "\n",
    "training_logs = {}\n",
    "\n",
    "print(\"Searching for training logs...\")\n",
    "print()\n",
    "\n",
    "# Look for DQN logs\n",
    "dqn_log_patterns = ['dqn_cartpole', 'dqn_maze', 'DQN']\n",
    "ppo_log_patterns = ['ppo_cartpole', 'ppo_maze', 'PPO']\n",
    "\n",
    "if LOG_DIR.exists():\n",
    "    for log_file in LOG_DIR.glob('**/*'):\n",
    "        if log_file.is_file():\n",
    "            # Check if it's a DQN log\n",
    "            if any(pattern in str(log_file) for pattern in dqn_log_patterns):\n",
    "                print(f\"Found DQN log: {log_file}\")\n",
    "                training_logs['DQN'] = log_file\n",
    "            # Check if it's a PPO log\n",
    "            elif any(pattern in str(log_file) for pattern in ppo_log_patterns):\n",
    "                print(f\"Found PPO log: {log_file}\")\n",
    "                training_logs['PPO'] = log_file\n",
    "else:\n",
    "    print(f\"⚠ Log directory not found: {LOG_DIR}\")\n",
    "\n",
    "if not training_logs:\n",
    "    print(\"\\n⚠ No training logs found.\")\n",
    "    print(\"Training logs are typically saved during model training.\")\n",
    "    print(\"To generate logs, train models using TensorBoard logging.\")\n",
    "else:\n",
    "    print(f\"\\n✓ Found {len(training_logs)} training log(s)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parse and Plot Learning Curves\n",
    "\n",
    "Note: This section requires TensorBoard log files. If logs are not available, this section will be skipped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if training_logs:\n",
    "    try:\n",
    "        from tensorboard.backend.event_processing import event_accumulator\n",
    "        \n",
    "        # Parse training logs\n",
    "        learning_data = {}\n",
    "        \n",
    "        for algo_name, log_path in training_logs.items():\n",
    "            print(f\"\\nParsing {algo_name} logs...\")\n",
    "            \n",
    "            # Load TensorBoard event file\n",
    "            ea = event_accumulator.EventAccumulator(str(log_path))\n",
    "            ea.Reload()\n",
    "            \n",
    "            # Extract available metrics\n",
    "            available_tags = ea.Tags()['scalars']\n",
    "            print(f\"  Available metrics: {available_tags}\")\n",
    "            \n",
    "            # Try to extract reward/return data\n",
    "            reward_tags = [tag for tag in available_tags if 'reward' in tag.lower() or 'return' in tag.lower()]\n",
    "            \n",
    "            if reward_tags:\n",
    "                # Use the first reward tag found\n",
    "                reward_tag = reward_tags[0]\n",
    "                events = ea.Scalars(reward_tag)\n",
    "                \n",
    "                steps = [e.step for e in events]\n",
    "                values = [e.value for e in events]\n",
    "                \n",
    "                learning_data[algo_name] = {\n",
    "                    'steps': steps,\n",
    "                    'rewards': values,\n",
    "                    'metric': reward_tag\n",
    "                }\n",
    "                \n",
    "                print(f\"  ✓ Extracted {len(steps)} data points\")\n",
    "            else:\n",
    "                print(f\"  ⚠ No reward metrics found\")\n",
    "        \n",
    "        # Plot learning curves\n",
    "        if learning_data:\n",
    "            plt.figure(figsize=(12, 6))\n",
    "            \n",
    "            colors = {'DQN': '#3498db', 'PPO': '#2ecc71'}\n",
    "            \n",
    "            for algo_name, data in learning_data.items():\n",
    "                color = colors.get(algo_name, '#95a5a6')\n",
    "                plt.plot(data['steps'], data['rewards'], \n",
    "                        label=f\"{algo_name} ({data['metric']})\",\n",
    "                        color=color, linewidth=2, alpha=0.8)\n",
    "            \n",
    "            plt.xlabel('Training Steps', fontsize=12)\n",
    "            plt.ylabel('Reward / Return', fontsize=12)\n",
    "            plt.title('Learning Curves: DQN vs PPO', fontsize=14, fontweight='bold')\n",
    "            plt.legend(fontsize=11)\n",
    "            plt.grid(alpha=0.3)\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "            \n",
    "            print(\"\\n✓ Learning curves plotted\")\n",
    "            \n",
    "            # Analysis\n",
    "            print(\"\\nLearning Curve Analysis:\")\n",
    "            for algo_name, data in learning_data.items():\n",
    "                final_reward = data['rewards'][-1] if data['rewards'] else 0\n",
    "                max_reward = max(data['rewards']) if data['rewards'] else 0\n",
    "                print(f\"\\n{algo_name}:\")\n",
    "                print(f\"  Final reward: {final_reward:.2f}\")\n",
    "                print(f\"  Max reward: {max_reward:.2f}\")\n",
    "                print(f\"  Training steps: {data['steps'][-1] if data['steps'] else 0:,}\")\n",
    "        \n",
    "    except ImportError:\n",
    "        print(\"\\n⚠ TensorBoard not installed. Cannot parse training logs.\")\n",
    "        print(\"Install with: pip install tensorboard\")\n",
    "    except Exception as e:\n",
    "        print(f\"\\n⚠ Error parsing training logs: {e}\")\n",
    "        print(\"Training logs may be in an incompatible format.\")\n",
    "else:\n",
    "    print(\"\\n⚠ Skipping learning curve comparison (no training logs available)\")\n",
    "    print(\"\\nTo generate learning curves:\")\n",
    "    print(\"1. Train models with TensorBoard logging enabled\")\n",
    "    print(\"2. Save logs to ml/experiments/logs/\")\n",
    "    print(\"3. Re-run this notebook\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Summary and Conclusions\n",
    "\n",
    "Let's consolidate all our findings into a comprehensive summary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comprehensive Comparison Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive summary table\n",
    "if evaluation_results and performance_results:\n",
    "    summary_data = []\n",
    "    \n",
    "    for agent_name in agents.keys():\n",
    "        if agent_name in evaluation_results and agent_name in performance_results:\n",
    "            eval_res = evaluation_results[agent_name]\n",
    "            perf_res = performance_results[agent_name]\n",
    "            \n",
    "            summary_data.append({\n",
    "                'Agent': agent_name,\n",
    "                'Mean Reward': f\"{eval_res['mean_reward']:.2f}\",\n",
    "                'Success Rate': f\"{eval_res['success_rate']*100:.1f}%\",\n",
    "                'Episode Length': f\"{eval_res['mean_episode_length']:.1f}\",\n",
    "                'Inference Time (ms)': f\"{perf_res['inference_time_ms']:.4f}\",\n",
    "                'Parameters': f\"{perf_res['parameter_count']:,}\",\n",
    "                'Memory (MB)': f\"{perf_res['memory_footprint_mb']:.2f}\"\n",
    "            })\n",
    "    \n",
    "    if summary_data:\n",
    "        df_summary = pd.DataFrame(summary_data)\n",
    "        \n",
    "        print(\"=\"*100)\n",
    "        print(\"COMPREHENSIVE ALGORITHM COMPARISON SUMMARY\")\n",
    "        print(\"=\"*100)\n",
    "        print()\n",
    "        print(df_summary.to_string(index=False))\n",
    "        print()\n",
    "        print(\"=\"*100)\n",
    "    else:\n",
    "        print(\"⚠ Insufficient data for summary table\")\n",
    "else:\n",
    "    print(\"⚠ Complete evaluation and performance data needed for summary\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Findings and Interpretation\n",
    "\n",
    "### Performance Comparison:\n",
    "\n",
    "**Random Agent (Baseline):**\n",
    "- Provides a baseline for comparison\n",
    "- No learning, purely random actions\n",
    "- Expected to have the worst performance metrics\n",
    "- Fastest inference (no computation required)\n",
    "- Zero parameters and memory footprint\n",
    "\n",
    "**DQN (Deep Q-Network):**\n",
    "- Value-based reinforcement learning\n",
    "- Learns Q-values for state-action pairs\n",
    "- Single Q-Network architecture\n",
    "- Deterministic policy (argmax over Q-values)\n",
    "- Good for discrete action spaces\n",
    "- Moderate inference speed\n",
    "- Smaller model size compared to PPO\n",
    "\n",
    "**PPO (Proximal Policy Optimization):**\n",
    "- Policy-based reinforcement learning\n",
    "- Learns policy directly (Actor-Critic)\n",
    "- Dual network architecture (Actor + Critic)\n",
    "- Stochastic policy (samples from distribution)\n",
    "- Works with both discrete and continuous actions\n",
    "- Slightly slower inference (two networks)\n",
    "- Larger model size due to dual networks\n",
    "\n",
    "### Algorithm Selection Guide:\n",
    "\n",
    "**Choose Random Agent when:**\n",
    "- You need a baseline for comparison\n",
    "- Testing environment setup\n",
    "- Initial exploration phase\n",
    "\n",
    "**Choose DQN when:**\n",
    "- Working with discrete action spaces\n",
    "- Need deterministic policies\n",
    "- Sample efficiency is important\n",
    "- Model size constraints exist\n",
    "- Inference speed is critical\n",
    "\n",
    "**Choose PPO when:**\n",
    "- Working with continuous action spaces\n",
    "- Need stochastic policies\n",
    "- Stability is more important than speed\n",
    "- General-purpose RL solution needed\n",
    "- Multi-agent scenarios\n",
    "\n",
    "### Trade-offs Summary:\n",
    "\n",
    "| Aspect | Random | DQN | PPO |\n",
    "|--------|--------|-----|-----|\n",
    "| **Performance** | Worst | Excellent | Excellent |\n",
    "| **Inference Speed** | Fastest | Fast | Moderate |\n",
    "| **Model Size** | None | Small | Medium |\n",
    "| **Action Space** | Any | Discrete | Both |\n",
    "| **Stability** | N/A | Moderate | High |\n",
    "| **Sample Efficiency** | N/A | High | Medium |\n",
    "| **Ease of Tuning** | N/A | Moderate | Easy |\n",
    "\n",
    "### Practical Recommendations:\n",
    "\n",
    "1. **For Production Deployment:**\n",
    "   - If discrete actions: DQN (faster, smaller)\n",
    "   - If continuous actions: PPO (only option)\n",
    "   - Consider inference latency requirements\n",
    "\n",
    "2. **For Research/Experimentation:**\n",
    "   - Start with PPO (more stable, easier to tune)\n",
    "   - Try DQN if sample efficiency is critical\n",
    "   - Always compare against Random baseline\n",
    "\n",
    "3. **For Resource-Constrained Environments:**\n",
    "   - Prefer DQN (smaller model, faster inference)\n",
    "   - Consider model compression techniques\n",
    "   - Profile actual memory and compute usage\n",
    "\n",
    "4. **For Learning:**\n",
    "   - Understand Random agent as baseline\n",
    "   - Study DQN for value-based methods\n",
    "   - Study PPO for policy-based methods\n",
    "   - Compare architectures and behaviors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization: Overall Comparison Radar Chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if evaluation_results and performance_results and len(evaluation_results) > 1:\n",
    "    import numpy as np\n",
    "    from math import pi\n",
    "    \n",
    "    # Prepare data for radar chart\n",
    "    categories = ['Performance\\n(Reward)', 'Success\\nRate', 'Inference\\nSpeed', 'Model\\nSize']\n",
    "    N = len(categories)\n",
    "    \n",
    "    # Normalize metrics to 0-1 scale (higher is better)\n",
    "    agent_data = {}\n",
    "    \n",
    "    for agent_name in evaluation_results.keys():\n",
    "        if agent_name in performance_results:\n",
    "            eval_res = evaluation_results[agent_name]\n",
    "            perf_res = performance_results[agent_name]\n",
    "            \n",
    "            agent_data[agent_name] = {\n",
    "                'reward': eval_res['mean_reward'],\n",
    "                'success': eval_res['success_rate'],\n",
    "                'speed': perf_res['inference_time_ms'],\n",
    "                'size': perf_res['parameter_count']\n",
    "            }\n",
    "    \n",
    "    # Normalize\n",
    "    max_reward = max(d['reward'] for d in agent_data.values()) or 1\n",
    "    max_speed = max(d['speed'] for d in agent_data.values()) or 1\n",
    "    max_size = max(d['size'] for d in agent_data.values()) or 1\n",
    "    \n",
    "    normalized_data = {}\n",
    "    for agent_name, data in agent_data.items():\n",
    "        normalized_data[agent_name] = [\n",
    "            data['reward'] / max_reward,  # Performance (higher is better)\n",
    "            data['success'],  # Success rate (already 0-1)\n",
    "            1 - (data['speed'] / max_speed),  # Speed (invert: lower time is better)\n",
    "            1 - (data['size'] / max_size) if max_size > 0 else 1  # Size (invert: smaller is better)\n",
    "        ]\n",
    "    \n",
    "    # Create radar chart\n",
    "    angles = [n / float(N) * 2 * pi for n in range(N)]\n",
    "    angles += angles[:1]\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(10, 10), subplot_kw=dict(projection='polar'))\n",
    "    \n",
    "    colors = {'Random': '#e74c3c', 'DQN': '#3498db', 'PPO': '#2ecc71'}\n",
    "    \n",
    "    for agent_name, values in normalized_data.items():\n",
    "        values += values[:1]  # Complete the circle\n",
    "        color = colors.get(agent_name, '#95a5a6')\n",
    "        ax.plot(angles, values, 'o-', linewidth=2, label=agent_name, color=color)\n",
    "        ax.fill(angles, values, alpha=0.15, color=color)\n",
    "    \n",
    "    ax.set_xticks(angles[:-1])\n",
    "    ax.set_xticklabels(categories, size=12)\n",
    "    ax.set_ylim(0, 1)\n",
    "    ax.set_yticks([0.2, 0.4, 0.6, 0.8, 1.0])\n",
    "    ax.set_yticklabels(['0.2', '0.4', '0.6', '0.8', '1.0'], size=10)\n",
    "    ax.grid(True)\n",
    "    \n",
    "    plt.legend(loc='upper right', bbox_to_anchor=(1.3, 1.1), fontsize=12)\n",
    "    plt.title('Overall Algorithm Comparison\\n(Normalized Metrics)', \n",
    "             size=16, fontweight='bold', pad=20)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\n✓ Radar chart generated\")\n",
    "    print(\"\\nNote: All metrics normalized to 0-1 scale where higher is better.\")\n",
    "    print(\"Inference speed and model size are inverted (smaller is better).\")\n",
    "else:\n",
    "    print(\"⚠ Multiple agents with complete data needed for radar chart\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This notebook has provided a comprehensive comparison of three reinforcement learning approaches:\n",
    "\n",
    "1. **Random Agent**: Baseline with no learning\n",
    "2. **DQN**: Value-based method with Q-learning\n",
    "3. **PPO**: Policy-based method with Actor-Critic\n",
    "\n",
    "### Main Takeaways:\n",
    "\n",
    "- Both DQN and PPO significantly outperform random baseline\n",
    "- DQN offers faster inference and smaller model size\n",
    "- PPO provides more flexibility and stability\n",
    "- Choice depends on specific requirements (action space, speed, stability)\n",
    "\n",
    "### Next Steps:\n",
    "\n",
    "1. Experiment with different hyperparameters\n",
    "2. Try other RL algorithms (A3C, SAC, TD3)\n",
    "3. Test on more complex environments\n",
    "4. Implement custom reward shaping\n",
    "5. Deploy best-performing model\n",
    "\n",
    "---\n",
    "\n",
    "*This comparison notebook is part of the RL Algorithm Comparison feature.*\n",
    "*For questions or improvements, refer to the project documentation.*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}